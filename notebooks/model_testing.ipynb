{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc4632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "from PIL import UnidentifiedImageError\n",
    "import requests\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\" # get rid of all tensorflow warnings\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from gensim.models import Word2Vec\n",
    "import datetime as dt\n",
    "import time\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "import pickle\n",
    "import string\n",
    "#### contraction ####\n",
    "### nltk ###\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.utils import pad_sequences\n",
    "#### might be wrong\n",
    "\n",
    "\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "word2vec = Word2Vec.load(\"../models/w2v_150k\")\n",
    "vec_size = 40\n",
    "max_length = 10\n",
    "\n",
    "\n",
    "##Davids timestamper\n",
    "\n",
    "\n",
    "def basic(original_df,keep_timestamp=False):\n",
    "    \"\"\"\n",
    "    Transforms 'time_stamp' column from df into individual components 'year',\n",
    "    'month','day','weekday','hour','minute'\n",
    "    \"\"\"\n",
    "    df = original_df.copy()\n",
    "\n",
    "    if 'time_stamp' not in df.columns:\n",
    "        raise ValueError(\"df has no column named 'time_stamp'\")\n",
    "    df['time_stamp'] = pd.to_datetime(df['time_stamp'], unit='s')\n",
    "\n",
    "    df['year'] = df.time_stamp.dt.year\n",
    "    df['month'] = df.time_stamp.dt.month\n",
    "    df['day'] = df.time_stamp.dt.day\n",
    "    df['weekday'] = df.time_stamp.dt.weekday\n",
    "    df['hour'] = df.time_stamp.dt.hour\n",
    "    df['minute'] = df.time_stamp.dt.minute\n",
    "\n",
    "    if keep_timestamp is False:\n",
    "        df = df.drop(columns='time_stamp')\n",
    "    return df\n",
    "\n",
    "def cyclize(original_df):\n",
    "    \"\"\"\n",
    "    Transforms columns named 'month','day','hour','minute' into sin and cos\n",
    "    cyclic values for use with machine learning models\n",
    "    \"\"\"\n",
    "    df = original_df.copy()\n",
    "\n",
    "    need_list = ['month','day','hour','minute']\n",
    "    max_dict = {\n",
    "        'month':12,\n",
    "        'day': 31,\n",
    "        'hour': 23,\n",
    "        'minute': 59\n",
    "    }\n",
    "\n",
    "    for column in need_list:\n",
    "        if column in df.columns:\n",
    "            def sin_trans(number):\n",
    "                return math.sin(number * (2. * math.pi / max_dict[column]))\n",
    "            def cos_trans(number):\n",
    "                return math.cos(number * (2. * math.pi / max_dict[column]))\n",
    "            df['sin_' + column] = df[column].apply(sin_trans)\n",
    "            df['cos_' + column] = df[column].apply(cos_trans)\n",
    "            df = df.drop(columns=column, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode_weekday(original_df, keep_weekday_column=False):\n",
    "    \"\"\"\n",
    "    OneHotEncodes column from df column named 'weekday'\n",
    "    \"\"\"\n",
    "    df = original_df.copy()\n",
    "\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    df_wkdy = pd.DataFrame(enc.fit_transform(df[['weekday']]).toarray())\n",
    "    df = pd.concat([df.reset_index(), df_wkdy], axis=1)\n",
    "    df = df.set_index('index')\n",
    "    if keep_weekday_column==False:\n",
    "        df = df.drop('weekday', axis=1)\n",
    "    return df\n",
    "\n",
    "def transform_timestamp(original_df):\n",
    "    \"\"\"\n",
    "    Takes 'time_stamp' column from df and returns df preprocessed and\n",
    "    ready for machine learning\n",
    "    \"\"\"\n",
    "    df = original_df.copy()\n",
    "    df = basic(df)\n",
    "    df = cyclize(df)\n",
    "    df = encode_weekday(df)\n",
    "    if 'year' in df.columns:\n",
    "        scaler = MinMaxScaler()\n",
    "        df['year'] = scaler.fit_transform(df[['year']].copy())\n",
    "    return df\n",
    "\n",
    "###Binglins NLP\n",
    "\n",
    "\n",
    "def count_len(text):\n",
    "    # add a column to the dataframe, showing the length of each 'title'\n",
    "    text = text.split(' ')\n",
    "    length = len(text)\n",
    "    return length\n",
    "\n",
    "def preprocessing(text, contraction_mapping=CONTRACTION_MAP):\n",
    "\n",
    "    # 1. Expand Contractions\n",
    "    \"\"\"Expand the contractions in English. e.g. I'm ==> I am\"\"\"\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "\n",
    "    # 2. Basic Cleaning\n",
    "    sentence = expanded_text.lower()\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "    ## punctuation dictionary ##\n",
    "    my_punc = string.punctuation\n",
    "    my_punc += '—'\n",
    "    my_punc += '“”’'\n",
    "    ############################\n",
    "    for punctuation in my_punc:\n",
    "        sentence = sentence.replace(punctuation, '')\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    # 2. Remove Stopwords\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    remove_s = \" \".join([word for word in str(sentence).split() if word not in STOPWORDS])\n",
    "\n",
    "    # 3. Word Tokenize\n",
    "    word_tokens = word_tokenize(remove_s)\n",
    "\n",
    "    # 4. Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_n = [lemmatizer.lemmatize(word,pos='n') for word in word_tokens]\n",
    "    lemmatized_v = [lemmatizer.lemmatize(word,pos='v') for word in lemmatized_n]\n",
    "    return lemmatized_v\n",
    "\n",
    "def embedding(text,word2vec):\n",
    "    # 5. Embedding\n",
    "    word2vec,\n",
    "    wv = word2vec.wv\n",
    "    to_array = []\n",
    "    for word in text:\n",
    "        if word in wv.key_to_index:\n",
    "            to_array.append(wv[word])\n",
    "    return np.array(to_array)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#final preprocessor\n",
    "def preprocess(data):\n",
    "    df = data\n",
    "    dataframe=[]\n",
    "\n",
    "    for i in range(6):\n",
    "    # get the path/directory\n",
    "        folder_dir = f\"../raw_data/images/category_{i}\"\n",
    "        for images in os.listdir(folder_dir):\n",
    "            yeet = []\n",
    "            path = os.path.join(folder_dir, images)\n",
    "            image = Image.open(path)\n",
    "            id_, size, upvotes = images.replace(\".png\", \"\").split(\"_\")\n",
    "            yeet.append(id_)\n",
    "            yeet.append(size)\n",
    "            arr = np.array(image)\n",
    "            try:\n",
    "                A,B,C = arr.shape\n",
    "                if C == 4:\n",
    "                    arr = arr[:,:,:3]\n",
    "                    image = Image.fromarray(arr)\n",
    "                    image.save(path)\n",
    "                yeet.append(path)\n",
    "                yeet.append(i)\n",
    "                dataframe.append(yeet)\n",
    "            except ValueError:\n",
    "                os.remove(path)\n",
    "    data_arrys =pd.DataFrame(dataframe)\n",
    "    data_arrys.rename(columns={0 :'id', 1:\"size\", 2:\"image_path\", 3:\"y_cat\"}, inplace=True)\n",
    "    #merge\n",
    "    df = pd.merge(data_arrys, df)\n",
    "    df = transform_timestamp(df)\n",
    "    ### Add column: length of Title\n",
    "    df['title_len']=df['title'].apply(count_len)\n",
    "    ### Preprocessing ###\n",
    "    df['preprocessing'] = df['title'].apply(lambda sentence: preprocessing(sentence))\n",
    "    ## Embedding ###\n",
    "\n",
    "    #word2vec = Word2Vec(sentences=df[\"preprocessing\"], vector_size=vec_size, min_count=10, window=4)####CHANGE DIS\n",
    "    df['embedding'] = df['preprocessing'].apply(lambda x: embedding(x,word2vec))\n",
    "    ### Padding ###\n",
    "\n",
    "    t = pad_sequences(df['embedding'], dtype='float32', padding='post', maxlen=max_length)\n",
    "    tes = []\n",
    "    for i in range(t.shape[0]):\n",
    "        tes.append(t[i])\n",
    "    df['padding'] = tes\n",
    "\n",
    "    X_im = df[\"image_path\"]\n",
    "    df[\"size\"] = df[\"size\"].apply(lambda x : int(x))\n",
    "    X_im_size = df[\"size\"]\n",
    "    X_timestep = df[[\"year\", \"sin_month\", \"cos_month\", \"sin_day\", \"cos_day\", \"sin_hour\", \"cos_hour\", \"sin_minute\",\"cos_minute\", 0, 1, 2, 3, 4, 5, 6]].values\n",
    "    X_t_size = df[\"title_len\"]\n",
    "\n",
    "\n",
    "    X_NLP = df[\"padding\"]\n",
    "    X_NLP =[np.expand_dims(x, axis=0) for x in X_NLP]\n",
    "    X_NLP = np.array(X_NLP)\n",
    "    X_NLP = np.concatenate(X_NLP, axis = 0)\n",
    "\n",
    "\n",
    "    df[\"y_cat\"] = df[\"y_cat\"].astype(\"string\")\n",
    "    y = df[\"y_cat\"]\n",
    "    return { \"input_Im\": X_im, \"input_size_im\": X_im_size, \"input_size_title\": X_t_size,\"input_timestep\":X_timestep,\"input_NLP\": X_NLP}, y, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93ac229c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30289 validated image filenames belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14710/3720163661.py:126: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  X_im_size = dff[\"id\"][idx:end].to_numpy()\n",
      "/tmp/ipykernel_14710/3720163661.py:128: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  X_t_size = dff[\"title_len\"][idx:end].to_numpy()\n",
      "/tmp/ipykernel_14710/3720163661.py:129: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  X_NLP = dff[\"padding\"][idx:end]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not build a TypeSpec for            id      size                                         image_path  \\\nindex                                                                        \n9487   yrfl3b   4071000  ../raw_data/images/category_1/yrfl3b_4071000_8...   \n11700  ijt17q  12192768  ../raw_data/images/category_2/ijt17q_12192768_...   \n15404  kb94iz   1687500  ../raw_data/images/category_3/kb94iz_1687500_9...   \n11965  uud0zf  12192768  ../raw_data/images/category_2/uud0zf_12192768_...   \n15501  y0hd5m   1432730  ../raw_data/images/category_3/y0hd5m_1432730_6...   \n...       ...       ...                                                ...   \n5404   bdsvfe  15925248  ../raw_data/images/category_1/bdsvfe_15925248_...   \n15426  egypv1   3145728  ../raw_data/images/category_3/egypv1_3145728_7...   \n8229   6bjf4t   7990272  ../raw_data/images/category_1/6bjf4t_7990272_1...   \n27420  9a1x6n  10912062  ../raw_data/images/category_5/9a1x6n_10912062_...   \n914    ah27oz  12918992  ../raw_data/images/category_0/ah27oz_12918992_...   \n\n      y_cat                                              title  \\\nindex                                                            \n9487      1                   my dog likes to stare at the sun   \n11700     2         He may be a old boy, but he’s my best boy.   \n15404     3  Chloe looking like she’s about to sell you lif...   \n11965     2                               Abbie the party pup.   \n15501     3      My favorite old boy Sabre the Dogo Argentino.   \n...     ...                                                ...   \n5404      1                                 Djuka the girl 😊😊😊   \n15426     3      My Elder pirate watching for stupid squirrels   \n8229      1                                        Silly Mamas   \n27420     5  Love this cheeky little Kelpie pup. One out of...   \n914       0                                    Winter car ride   \n\n                                 Image_url  upvote_ratio  upvotes  upvote_cat  \\\nindex                                                                           \n9487   https://i.redd.it/44855v0v66z91.jpg          0.91        8           1   \n11700  https://i.redd.it/8q3h81l49ak51.jpg          0.95       24           2   \n15404  https://i.redd.it/pfof1l3cxl461.jpg          0.98       92           3   \n11965  https://i.redd.it/svnovuc0jq091.jpg          0.91       20           2   \n15501  https://i.redd.it/l05uzjzgwzs91.jpg          0.95       60           3   \n...                                    ...           ...      ...         ...   \n5404        http://i.imgur.com/uRYmt3T.jpg          0.88       11           1   \n15426  https://i.redd.it/sn9w1n3vug741.jpg          0.98       72           3   \n8229    https://i.redd.it/5gxj1rcvrwxy.jpg          0.92       11           1   \n27420      https://i.imgur.com/9FGTbJp.jpg          0.98     2441           5   \n914          https://imgur.com/1xaOpHE.jpg          1.00        1           0   \n\n           year  ...    1    2    3    4    5    6  title_len  \\\nindex            ...                                            \n9487   1.000000  ...  0.0  0.0  1.0  0.0  0.0  0.0          8   \n11700  0.833333  ...  0.0  0.0  0.0  0.0  0.0  0.0         11   \n15404  0.833333  ...  0.0  0.0  0.0  1.0  0.0  0.0         10   \n11965  1.000000  ...  0.0  0.0  0.0  0.0  1.0  0.0          4   \n15501  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0          8   \n...         ...  ...  ...  ...  ...  ...  ...  ...        ...   \n5404   0.750000  ...  1.0  0.0  0.0  0.0  0.0  0.0          4   \n15426  0.750000  ...  0.0  0.0  0.0  0.0  0.0  1.0          7   \n8229   0.583333  ...  1.0  0.0  0.0  0.0  0.0  0.0          2   \n27420  0.666667  ...  0.0  0.0  0.0  1.0  0.0  0.0         22   \n914    0.750000  ...  0.0  0.0  1.0  0.0  0.0  0.0          3   \n\n                                           preprocessing  \\\nindex                                                      \n9487                             [dog, like, stare, sun]   \n11700                     [may, old, boy, he, best, boy]   \n15404   [chloe, look, like, shes, sell, life, insurance]   \n11965                                [abbie, party, pup]   \n15501       [favorite, old, boy, sabre, dogo, argentino]   \n...                                                  ...   \n5404                                  [djuka, girl, 😊😊😊]   \n15426           [elder, pirate, watch, stupid, squirrel]   \n8229                                       [silly, mama]   \n27420  [love, cheeky, little, kelpie, pup, one, litte...   \n914                                  [winter, car, ride]   \n\n                                               embedding  \\\nindex                                                      \n9487   [[0.35266894, -0.44480756, 1.2790529, 0.226411...   \n11700  [[0.82692224, -0.22039042, -0.8130363, 0.61339...   \n15404  [[-0.055271313, -0.13597453, -0.58476824, 0.03...   \n11965  [[0.45161363, -0.4011149, 0.20811391, -0.53929...   \n15501  [[0.2762756, 2.1512673, -0.026870538, -0.86067...   \n...                                                  ...   \n5404   [[1.2286396, -1.3288174, -0.24003081, -0.24175...   \n15426  [[-0.11328321, -0.026498873, -0.14387736, 0.14...   \n8229   [[-0.6187335, 0.494518, -0.12809905, 0.0371877...   \n27420  [[0.7073981, -0.1938361, -0.17172599, -0.85855...   \n914    [[0.45897824, -0.6914342, -0.65909344, -0.4955...   \n\n                                                 padding  \nindex                                                     \n9487   [[0.35266894, -0.44480756, 1.2790529, 0.226411...  \n11700  [[0.82692224, -0.22039042, -0.8130363, 0.61339...  \n15404  [[-0.055271313, -0.13597453, -0.58476824, 0.03...  \n11965  [[0.45161363, -0.4011149, 0.20811391, -0.53929...  \n15501  [[0.2762756, 2.1512673, -0.026870538, -0.86067...  \n...                                                  ...  \n5404   [[1.2286396, -1.3288174, -0.24003081, -0.24175...  \n15426  [[-0.11328321, -0.026498873, -0.14387736, 0.14...  \n8229   [[-0.6187335, 0.494518, -0.12809905, 0.0371877...  \n27420  [[-0.33520874, -0.34282863, -0.87791234, 0.106...  \n914    [[0.45897824, -0.6914342, -0.65909344, -0.4955...  \n\n[30289 rows x 29 columns] of unsupported type <class 'pandas.core.frame.DataFrame'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 176\u001b[0m\n\u001b[1;32m    173\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m--> 176\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [14], line 163\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_name, new, old_model)\u001b[0m\n\u001b[1;32m    161\u001b[0m X_dict, y, df \u001b[38;5;241m=\u001b[39m preprocess(df)\n\u001b[1;32m    162\u001b[0m GENERATOR \u001b[38;5;241m=\u001b[39m createGenerator(df)\n\u001b[0;32m--> 163\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m\u001b[49m\u001b[43mGENERATOR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m893\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m\u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \n\u001b[1;32m    171\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m#validation_data = GENERATOR_train\u001b[39;49;00m\n\u001b[1;32m    172\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Upvote_Model/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/Upvote_Model/lib/python3.10/site-packages/tensorflow/python/framework/type_spec.py:924\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    921\u001b[0m   logging\u001b[38;5;241m.\u001b[39mvlog(\n\u001b[1;32m    922\u001b[0m       \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m to tensor: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, e))\n\u001b[0;32m--> 924\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not build a TypeSpec for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    925\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a TypeSpec for            id      size                                         image_path  \\\nindex                                                                        \n9487   yrfl3b   4071000  ../raw_data/images/category_1/yrfl3b_4071000_8...   \n11700  ijt17q  12192768  ../raw_data/images/category_2/ijt17q_12192768_...   \n15404  kb94iz   1687500  ../raw_data/images/category_3/kb94iz_1687500_9...   \n11965  uud0zf  12192768  ../raw_data/images/category_2/uud0zf_12192768_...   \n15501  y0hd5m   1432730  ../raw_data/images/category_3/y0hd5m_1432730_6...   \n...       ...       ...                                                ...   \n5404   bdsvfe  15925248  ../raw_data/images/category_1/bdsvfe_15925248_...   \n15426  egypv1   3145728  ../raw_data/images/category_3/egypv1_3145728_7...   \n8229   6bjf4t   7990272  ../raw_data/images/category_1/6bjf4t_7990272_1...   \n27420  9a1x6n  10912062  ../raw_data/images/category_5/9a1x6n_10912062_...   \n914    ah27oz  12918992  ../raw_data/images/category_0/ah27oz_12918992_...   \n\n      y_cat                                              title  \\\nindex                                                            \n9487      1                   my dog likes to stare at the sun   \n11700     2         He may be a old boy, but he’s my best boy.   \n15404     3  Chloe looking like she’s about to sell you lif...   \n11965     2                               Abbie the party pup.   \n15501     3      My favorite old boy Sabre the Dogo Argentino.   \n...     ...                                                ...   \n5404      1                                 Djuka the girl 😊😊😊   \n15426     3      My Elder pirate watching for stupid squirrels   \n8229      1                                        Silly Mamas   \n27420     5  Love this cheeky little Kelpie pup. One out of...   \n914       0                                    Winter car ride   \n\n                                 Image_url  upvote_ratio  upvotes  upvote_cat  \\\nindex                                                                           \n9487   https://i.redd.it/44855v0v66z91.jpg          0.91        8           1   \n11700  https://i.redd.it/8q3h81l49ak51.jpg          0.95       24           2   \n15404  https://i.redd.it/pfof1l3cxl461.jpg          0.98       92           3   \n11965  https://i.redd.it/svnovuc0jq091.jpg          0.91       20           2   \n15501  https://i.redd.it/l05uzjzgwzs91.jpg          0.95       60           3   \n...                                    ...           ...      ...         ...   \n5404        http://i.imgur.com/uRYmt3T.jpg          0.88       11           1   \n15426  https://i.redd.it/sn9w1n3vug741.jpg          0.98       72           3   \n8229    https://i.redd.it/5gxj1rcvrwxy.jpg          0.92       11           1   \n27420      https://i.imgur.com/9FGTbJp.jpg          0.98     2441           5   \n914          https://imgur.com/1xaOpHE.jpg          1.00        1           0   \n\n           year  ...    1    2    3    4    5    6  title_len  \\\nindex            ...                                            \n9487   1.000000  ...  0.0  0.0  1.0  0.0  0.0  0.0          8   \n11700  0.833333  ...  0.0  0.0  0.0  0.0  0.0  0.0         11   \n15404  0.833333  ...  0.0  0.0  0.0  1.0  0.0  0.0         10   \n11965  1.000000  ...  0.0  0.0  0.0  0.0  1.0  0.0          4   \n15501  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0          8   \n...         ...  ...  ...  ...  ...  ...  ...  ...        ...   \n5404   0.750000  ...  1.0  0.0  0.0  0.0  0.0  0.0          4   \n15426  0.750000  ...  0.0  0.0  0.0  0.0  0.0  1.0          7   \n8229   0.583333  ...  1.0  0.0  0.0  0.0  0.0  0.0          2   \n27420  0.666667  ...  0.0  0.0  0.0  1.0  0.0  0.0         22   \n914    0.750000  ...  0.0  0.0  1.0  0.0  0.0  0.0          3   \n\n                                           preprocessing  \\\nindex                                                      \n9487                             [dog, like, stare, sun]   \n11700                     [may, old, boy, he, best, boy]   \n15404   [chloe, look, like, shes, sell, life, insurance]   \n11965                                [abbie, party, pup]   \n15501       [favorite, old, boy, sabre, dogo, argentino]   \n...                                                  ...   \n5404                                  [djuka, girl, 😊😊😊]   \n15426           [elder, pirate, watch, stupid, squirrel]   \n8229                                       [silly, mama]   \n27420  [love, cheeky, little, kelpie, pup, one, litte...   \n914                                  [winter, car, ride]   \n\n                                               embedding  \\\nindex                                                      \n9487   [[0.35266894, -0.44480756, 1.2790529, 0.226411...   \n11700  [[0.82692224, -0.22039042, -0.8130363, 0.61339...   \n15404  [[-0.055271313, -0.13597453, -0.58476824, 0.03...   \n11965  [[0.45161363, -0.4011149, 0.20811391, -0.53929...   \n15501  [[0.2762756, 2.1512673, -0.026870538, -0.86067...   \n...                                                  ...   \n5404   [[1.2286396, -1.3288174, -0.24003081, -0.24175...   \n15426  [[-0.11328321, -0.026498873, -0.14387736, 0.14...   \n8229   [[-0.6187335, 0.494518, -0.12809905, 0.0371877...   \n27420  [[0.7073981, -0.1938361, -0.17172599, -0.85855...   \n914    [[0.45897824, -0.6914342, -0.65909344, -0.4955...   \n\n                                                 padding  \nindex                                                     \n9487   [[0.35266894, -0.44480756, 1.2790529, 0.226411...  \n11700  [[0.82692224, -0.22039042, -0.8130363, 0.61339...  \n15404  [[-0.055271313, -0.13597453, -0.58476824, 0.03...  \n11965  [[0.45161363, -0.4011149, 0.20811391, -0.53929...  \n15501  [[0.2762756, 2.1512673, -0.026870538, -0.86067...  \n...                                                  ...  \n5404   [[1.2286396, -1.3288174, -0.24003081, -0.24175...  \n15426  [[-0.11328321, -0.026498873, -0.14387736, 0.14...  \n8229   [[-0.6187335, 0.494518, -0.12809905, 0.0371877...  \n27420  [[-0.33520874, -0.34282863, -0.87791234, 0.106...  \n914    [[0.45897824, -0.6914342, -0.65909344, -0.4955...  \n\n[30289 rows x 29 columns] of unsupported type <class 'pandas.core.frame.DataFrame'>."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Multiple Inputs usin https://machinelearningmastery.com/keras-functional-api-deep-learning/\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Masking\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Normalization\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.applications.resnet import ResNet50, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "#from final_preprocessor import preprocess\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def initialize_model():\n",
    "    #Image convolution branch\n",
    "    input_Im = Input(shape=(128,128,3), name=\"input_Im\")\n",
    "    conv1 = Conv2D(64, kernel_size=(3, 3),activation='relu')(input_Im)\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
    "    conv2 = Conv2D(32, kernel_size=(3, 3),activation='relu')(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "    conv3 = Conv2D(32, kernel_size=(3, 3),activation='relu')(pool2)\n",
    "    pool3 = MaxPooling2D(pool_size=(2,2))(conv3)\n",
    "    conv2 = Conv2D(16, kernel_size=(3, 3),activation='relu')(pool3)\n",
    "    pool4 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "    flat1 = Flatten()(pool4)\n",
    "\n",
    "    #image_size branch\n",
    "    input_size_im = Input(shape=(1,), name=\"input_size_im\")\n",
    "    hidden1 = Dense(1, activation='relu')(input_size_im)\n",
    "    flat2 = Flatten()(hidden1)\n",
    "\n",
    "\n",
    "\n",
    "    initializer = keras.initializers.VarianceScaling(scale=0.1, mode=\"fan_in\", distribution=\"uniform\")\n",
    "    #NLP branch\n",
    "    input_NLP = Input(shape = (10,40), name=\"input_NLP\")###dont know dim padded and embedded inputs\n",
    "    mask = Masking()(input_NLP)\n",
    "    lstm = LSTM(32, activation = \"tanh\",kernel_initializer=initializer, kernel_regularizer=\"l1\")(mask)\n",
    "    dense1 = Dense(20, activation = \"relu\")(lstm)\n",
    "    flat3 = Flatten()(dense1)\n",
    "\n",
    "    #title_size branch\n",
    "    input_size_title = Input(shape=(1,), name=\"input_size_title\")\n",
    "    layer1 = Dense(1, activation='relu')(input_size_title)\n",
    "    flat4 = Flatten()(layer1)\n",
    "\n",
    "\n",
    "    #normalizer = Normalization()\n",
    "    #normalizer.adapt(\"X_train\")\n",
    "    #davids timestep\n",
    "    input_timestep = Input(shape=(16,),name=\"input_timestep\")#dont know dims\n",
    "    norm = Normalization()(input_timestep)\n",
    "    step1 = Dense(32, activation='relu')(norm)\n",
    "    #drop1 = Dropout(0.3)(step1)\n",
    "    step2 = Dense(16, activation='relu')(step1)\n",
    "    #drop2 = Dropout(0.2)(step2)\n",
    "    step3 = Dense(8, activation='relu')(step2)\n",
    "    #drop3 = Dropout(0.2)(step3)\n",
    "    step4 = Dense(4, activation='relu')(step3)\n",
    "    #drop4 = Dropout(0.2)(step4)\n",
    "    #drop4 = Dropout(0.1)(step5)\n",
    "    flat5 = Flatten()(step4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #concat them\n",
    "    merge = concatenate([flat1, flat2, flat3, flat4, flat5])\n",
    "    final1 = Dense(128, activation='relu')(merge)\n",
    "    final2 = Dense(64, activation='relu')(final1)\n",
    "    final2 = Dense(16, activation='relu')(final1)\n",
    "    output = Dense(6, activation='softmax')(final2)\n",
    "\n",
    "\n",
    "    #final model\n",
    "    model = Model(inputs=[input_Im, input_size_im, input_NLP, input_size_title, input_timestep], outputs=output)\n",
    "\n",
    "    #print(model.summary())\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "def createGenerator(dff, batch_size=BATCH_SIZE):\n",
    "\n",
    "    # Shuffles the dataframe, and so the batches as well\n",
    "    dff = dff.sample(frac=1)\n",
    "\n",
    "    # Shuffle=False is EXTREMELY important to keep order of image and coord\n",
    "    flow = datagen.flow_from_dataframe(\n",
    "                                        dataframe=dff,\n",
    "                                        directory=None,\n",
    "                                        x_col=\"image_path\",\n",
    "                                        y_col=\"y_cat\",\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=False,\n",
    "                                        class_mode=\"categorical\",\n",
    "                                        target_size=(128,128),\n",
    "                                        seed=42\n",
    "                                      )\n",
    "    idx = 0\n",
    "    n = len(dff) - batch_size\n",
    "    batch = 0\n",
    "    while True :\n",
    "        # Get next batch of images\n",
    "        X1 = flow.next()\n",
    "        # idx to reach\n",
    "        end = idx + X1[0].shape[0]\n",
    "        # get next batch of lines from df\n",
    "        X_im_size = dff[\"size\"][idx:end].to_numpy()\n",
    "        X_timestep = dff[[\"year\", \"sin_month\", \"cos_month\", \"sin_day\", \"cos_day\", \"sin_hour\", \"cos_hour\", \"sin_minute\",\"cos_minute\", 0, 1, 2, 3, 4, 5, 6]][idx:end].to_numpy()\n",
    "        X_t_size = dff[\"title_len\"][idx:end].to_numpy()\n",
    "        X_NLP = dff[\"padding\"][idx:end]\n",
    "        X_NLP =[np.expand_dims(x, axis=0) for x in X_NLP]\n",
    "        X_NLP = np.array(X_NLP)\n",
    "        X_NLP = np.concatenate(X_NLP, axis = 0)\n",
    "        dff_verif = dff[idx:end]\n",
    "        # Updates the idx for the next batch\n",
    "        idx = end\n",
    "#         print(\"batch nb : \", batch, \",   batch_size : \", X1[0].shape[0])\n",
    "        batch+=1\n",
    "        # Checks if we are at the end of the dataframe\n",
    "        if idx==len(dff):\n",
    "#             print(\"END OF THE DATAFRAME\\n\")\n",
    "            idx = 0\n",
    "        y = X1[1]\n",
    "        X_im = X1[0]\n",
    "        # Yields the image, metadata & target batches\n",
    "        yield { \"input_Im\": X_im, \"input_size_im\": X_im_size, \"input_size_title\": X_t_size,\"input_timestep\":X_timestep,\"input_NLP\": X_NLP},y, dff\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model( model_name, new = True, old_model = \"Model_predictor\"):\n",
    "    if new:\n",
    "        model = initialize_model()\n",
    "    else:\n",
    "        model = keras.models.load_model(f'../models/{old_model}.h5')\n",
    "        pass\n",
    "\n",
    "    df = pd.read_csv('data/balanced_35k.csv', index_col=0)\n",
    "    X_dict, y, df = preprocess(df)\n",
    "    GENERATOR = createGenerator(df)\n",
    "    model.fit(\n",
    "    GENERATOR,\n",
    "    epochs=100,\n",
    "    batch_size = 32,\n",
    "    steps_per_epoch=893,\n",
    "    workers = 1,\n",
    "    use_multiprocessing=False,\n",
    "\n",
    "    #validation_data = GENERATOR_train\n",
    "    )\n",
    "    model.save(f'../models/{model_name}')\n",
    "    return model\n",
    "\n",
    "train_model(\"model_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c592ea3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 30289 entries, 0 to 30288\n",
      "Series name: size\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "30289 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 473.3 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/balanced_35k.csv', index_col=0)\n",
    "X_dict, y, df = preprocess(df)\n",
    "df.to_csv('data/processed df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d40862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>size</th>\n",
       "      <th>image_path</th>\n",
       "      <th>y_cat</th>\n",
       "      <th>title</th>\n",
       "      <th>Image_url</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>upvote_cat</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>title_len</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>embedding</th>\n",
       "      <th>padding</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>si6nl6</td>\n",
       "      <td>12192768</td>\n",
       "      <td>../raw_data/images/category_0/si6nl6_12192768_...</td>\n",
       "      <td>0</td>\n",
       "      <td>My derpy girl giving her best smile!</td>\n",
       "      <td>https://i.redd.it/hbbsqh4dlaf81.jpg</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>[derpy, girl, give, best, smile]</td>\n",
       "      <td>[[-0.7024669, 0.10465372, -0.10564256, 0.24819...</td>\n",
       "      <td>[[-0.7024669, 0.10465372, -0.10564256, 0.24819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g92pmt</td>\n",
       "      <td>4802000</td>\n",
       "      <td>../raw_data/images/category_0/g92pmt_4802000_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi, can everyone hear me?</td>\n",
       "      <td>https://imgur.com/rUlJ8tp.jpg</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>[hi, everyone, hear]</td>\n",
       "      <td>[[-2.284348, 0.698452, -0.954938, 0.1315268, 0...</td>\n",
       "      <td>[[-2.284348, 0.698452, -0.954938, 0.1315268, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pebvge</td>\n",
       "      <td>8911728</td>\n",
       "      <td>../raw_data/images/category_0/pebvge_8911728_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>My baby 💙</td>\n",
       "      <td>https://i.redd.it/ahw384qsmfk71.jpg</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>[baby, 💙]</td>\n",
       "      <td>[[-0.15399729, -0.40509552, -0.64985436, -0.13...</td>\n",
       "      <td>[[-0.15399729, -0.40509552, -0.64985436, -0.13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pdvq6k</td>\n",
       "      <td>12000000</td>\n",
       "      <td>../raw_data/images/category_0/pdvq6k_12000000_...</td>\n",
       "      <td>0</td>\n",
       "      <td>This is the best pic i have with my dog and i ...</td>\n",
       "      <td>https://i.redd.it/yvsw7ijjzak71.jpg</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>[best, pic, dog, also, post, elsewhere, spread...</td>\n",
       "      <td>[[1.3253559, 0.38933644, 0.7680881, -1.9209212...</td>\n",
       "      <td>[[1.3253559, 0.38933644, 0.7680881, -1.9209212...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bsc52h</td>\n",
       "      <td>2742336</td>\n",
       "      <td>../raw_data/images/category_0/bsc52h_2742336_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>Can you tell she likes “rides” ?</td>\n",
       "      <td>https://i.redd.it/jplh6vc5x2031.jpg</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>[tell, like, ride]</td>\n",
       "      <td>[[-0.39929333, 0.03881802, -0.06807098, -0.325...</td>\n",
       "      <td>[[-0.39929333, 0.03881802, -0.06807098, -0.325...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30284</th>\n",
       "      <td>qcw42m</td>\n",
       "      <td>12192768</td>\n",
       "      <td>../raw_data/images/category_5/qcw42m_12192768_...</td>\n",
       "      <td>5</td>\n",
       "      <td>It’s my birthday, I’m 3!</td>\n",
       "      <td>https://i.redd.it/g5sg320l2uu71.jpg</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1629</td>\n",
       "      <td>5</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>[birthday, im]</td>\n",
       "      <td>[[2.2755084, 0.8114666, -1.2593371, 0.95703113...</td>\n",
       "      <td>[[2.2755084, 0.8114666, -1.2593371, 0.95703113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30285</th>\n",
       "      <td>ggb9cw</td>\n",
       "      <td>12979200</td>\n",
       "      <td>../raw_data/images/category_5/ggb9cw_12979200_...</td>\n",
       "      <td>5</td>\n",
       "      <td>This is my doggy-dog Aya</td>\n",
       "      <td>https://i.redd.it/ij7s5gyb2px41.jpg</td>\n",
       "      <td>1.00</td>\n",
       "      <td>621</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>[doggydog, aya]</td>\n",
       "      <td>[[-0.32109588, -0.2716897, -0.7240568, 0.18267...</td>\n",
       "      <td>[[-0.32109588, -0.2716897, -0.7240568, 0.18267...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30286</th>\n",
       "      <td>m4je4e</td>\n",
       "      <td>6502860</td>\n",
       "      <td>../raw_data/images/category_5/m4je4e_6502860_8...</td>\n",
       "      <td>5</td>\n",
       "      <td>One of my favourite pictures of our little girl</td>\n",
       "      <td>https://i.redd.it/ezbjnkwg0wm61.jpg</td>\n",
       "      <td>1.00</td>\n",
       "      <td>834</td>\n",
       "      <td>5</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>[one, favourite, picture, little, girl]</td>\n",
       "      <td>[[0.19927058, 2.0016553, -0.5724674, 0.3240424...</td>\n",
       "      <td>[[0.19927058, 2.0016553, -0.5724674, 0.3240424...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30287</th>\n",
       "      <td>738bkx</td>\n",
       "      <td>2073600</td>\n",
       "      <td>../raw_data/images/category_5/738bkx_2073600_7...</td>\n",
       "      <td>5</td>\n",
       "      <td>Reddit, say hi to Gus</td>\n",
       "      <td>https://i.redd.it/db0wbl946uoz.jpg</td>\n",
       "      <td>0.99</td>\n",
       "      <td>723</td>\n",
       "      <td>5</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>[reddit, say, hi, gu]</td>\n",
       "      <td>[[-0.8507807, 1.2658457, -0.4720916, 0.5673523...</td>\n",
       "      <td>[[-0.8507807, 1.2658457, -0.4720916, 0.5673523...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30288</th>\n",
       "      <td>d07php</td>\n",
       "      <td>450560</td>\n",
       "      <td>../raw_data/images/category_5/d07php_450560_12...</td>\n",
       "      <td>5</td>\n",
       "      <td>very wholesome &lt;3</td>\n",
       "      <td>https://i.redd.it/rizypllnnuk31.jpg</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1223</td>\n",
       "      <td>5</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>[wholesome]</td>\n",
       "      <td>[[-0.015081029, 0.11564291, -0.036546487, 0.17...</td>\n",
       "      <td>[[-0.015081029, 0.11564291, -0.036546487, 0.17...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30289 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id      size                                         image_path  \\\n",
       "index                                                                        \n",
       "0      si6nl6  12192768  ../raw_data/images/category_0/si6nl6_12192768_...   \n",
       "1      g92pmt   4802000  ../raw_data/images/category_0/g92pmt_4802000_1...   \n",
       "2      pebvge   8911728  ../raw_data/images/category_0/pebvge_8911728_1...   \n",
       "3      pdvq6k  12000000  ../raw_data/images/category_0/pdvq6k_12000000_...   \n",
       "4      bsc52h   2742336  ../raw_data/images/category_0/bsc52h_2742336_1...   \n",
       "...       ...       ...                                                ...   \n",
       "30284  qcw42m  12192768  ../raw_data/images/category_5/qcw42m_12192768_...   \n",
       "30285  ggb9cw  12979200  ../raw_data/images/category_5/ggb9cw_12979200_...   \n",
       "30286  m4je4e   6502860  ../raw_data/images/category_5/m4je4e_6502860_8...   \n",
       "30287  738bkx   2073600  ../raw_data/images/category_5/738bkx_2073600_7...   \n",
       "30288  d07php    450560  ../raw_data/images/category_5/d07php_450560_12...   \n",
       "\n",
       "      y_cat                                              title  \\\n",
       "index                                                            \n",
       "0         0               My derpy girl giving her best smile!   \n",
       "1         0                          Hi, can everyone hear me?   \n",
       "2         0                                          My baby 💙   \n",
       "3         0  This is the best pic i have with my dog and i ...   \n",
       "4         0                   Can you tell she likes “rides” ?   \n",
       "...     ...                                                ...   \n",
       "30284     5                           It’s my birthday, I’m 3!   \n",
       "30285     5                           This is my doggy-dog Aya   \n",
       "30286     5    One of my favourite pictures of our little girl   \n",
       "30287     5                              Reddit, say hi to Gus   \n",
       "30288     5                                  very wholesome <3   \n",
       "\n",
       "                                 Image_url  upvote_ratio  upvotes  upvote_cat  \\\n",
       "index                                                                           \n",
       "0      https://i.redd.it/hbbsqh4dlaf81.jpg          1.00        1           0   \n",
       "1            https://imgur.com/rUlJ8tp.jpg          1.00        1           0   \n",
       "2      https://i.redd.it/ahw384qsmfk71.jpg          1.00        1           0   \n",
       "3      https://i.redd.it/yvsw7ijjzak71.jpg          1.00        1           0   \n",
       "4      https://i.redd.it/jplh6vc5x2031.jpg          1.00        1           0   \n",
       "...                                    ...           ...      ...         ...   \n",
       "30284  https://i.redd.it/g5sg320l2uu71.jpg          0.99     1629           5   \n",
       "30285  https://i.redd.it/ij7s5gyb2px41.jpg          1.00      621           5   \n",
       "30286  https://i.redd.it/ezbjnkwg0wm61.jpg          1.00      834           5   \n",
       "30287   https://i.redd.it/db0wbl946uoz.jpg          0.99      723           5   \n",
       "30288  https://i.redd.it/rizypllnnuk31.jpg          0.99     1223           5   \n",
       "\n",
       "           year  ...    1    2    3    4    5    6  title_len  \\\n",
       "index            ...                                            \n",
       "0      1.000000  ...  1.0  0.0  0.0  0.0  0.0  0.0          7   \n",
       "1      0.833333  ...  0.0  0.0  0.0  0.0  0.0  0.0          5   \n",
       "2      0.916667  ...  0.0  0.0  0.0  0.0  0.0  0.0          3   \n",
       "3      0.916667  ...  0.0  0.0  0.0  0.0  0.0  1.0         22   \n",
       "4      0.750000  ...  0.0  0.0  0.0  1.0  0.0  0.0          7   \n",
       "...         ...  ...  ...  ...  ...  ...  ...  ...        ...   \n",
       "30284  0.916667  ...  0.0  0.0  1.0  0.0  0.0  0.0          5   \n",
       "30285  0.833333  ...  0.0  0.0  0.0  0.0  1.0  0.0          5   \n",
       "30286  0.916667  ...  0.0  0.0  0.0  0.0  0.0  1.0          9   \n",
       "30287  0.583333  ...  0.0  0.0  0.0  1.0  0.0  0.0          5   \n",
       "30288  0.750000  ...  0.0  0.0  1.0  0.0  0.0  0.0          3   \n",
       "\n",
       "                                           preprocessing  \\\n",
       "index                                                      \n",
       "0                       [derpy, girl, give, best, smile]   \n",
       "1                                   [hi, everyone, hear]   \n",
       "2                                              [baby, 💙]   \n",
       "3      [best, pic, dog, also, post, elsewhere, spread...   \n",
       "4                                     [tell, like, ride]   \n",
       "...                                                  ...   \n",
       "30284                                     [birthday, im]   \n",
       "30285                                    [doggydog, aya]   \n",
       "30286            [one, favourite, picture, little, girl]   \n",
       "30287                              [reddit, say, hi, gu]   \n",
       "30288                                        [wholesome]   \n",
       "\n",
       "                                               embedding  \\\n",
       "index                                                      \n",
       "0      [[-0.7024669, 0.10465372, -0.10564256, 0.24819...   \n",
       "1      [[-2.284348, 0.698452, -0.954938, 0.1315268, 0...   \n",
       "2      [[-0.15399729, -0.40509552, -0.64985436, -0.13...   \n",
       "3      [[1.3253559, 0.38933644, 0.7680881, -1.9209212...   \n",
       "4      [[-0.39929333, 0.03881802, -0.06807098, -0.325...   \n",
       "...                                                  ...   \n",
       "30284  [[2.2755084, 0.8114666, -1.2593371, 0.95703113...   \n",
       "30285  [[-0.32109588, -0.2716897, -0.7240568, 0.18267...   \n",
       "30286  [[0.19927058, 2.0016553, -0.5724674, 0.3240424...   \n",
       "30287  [[-0.8507807, 1.2658457, -0.4720916, 0.5673523...   \n",
       "30288  [[-0.015081029, 0.11564291, -0.036546487, 0.17...   \n",
       "\n",
       "                                                 padding  \n",
       "index                                                     \n",
       "0      [[-0.7024669, 0.10465372, -0.10564256, 0.24819...  \n",
       "1      [[-2.284348, 0.698452, -0.954938, 0.1315268, 0...  \n",
       "2      [[-0.15399729, -0.40509552, -0.64985436, -0.13...  \n",
       "3      [[1.3253559, 0.38933644, 0.7680881, -1.9209212...  \n",
       "4      [[-0.39929333, 0.03881802, -0.06807098, -0.325...  \n",
       "...                                                  ...  \n",
       "30284  [[2.2755084, 0.8114666, -1.2593371, 0.95703113...  \n",
       "30285  [[-0.32109588, -0.2716897, -0.7240568, 0.18267...  \n",
       "30286  [[0.19927058, 2.0016553, -0.5724674, 0.3240424...  \n",
       "30287  [[-0.8507807, 1.2658457, -0.4720916, 0.5673523...  \n",
       "30288  [[-0.015081029, 0.11564291, -0.036546487, 0.17...  \n",
       "\n",
       "[30289 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e21f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR = createGenerator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b8db445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30289 validated image filenames belonging to 6 classes.\n",
      "['uw0k0x' 'c58ibi' 'g6zfl7' 'em1fsi' 'csjwb9' 'jco6p4' '8a4kzw' 'qj2aug'\n",
      " 'pfwhx8' 'cg3nfn' 'affoba' 'n1wt3l' 'cq5k14' 'vl15lx' 'ytrvv0' 'ad78y0'\n",
      " 'swlm4y' '7hvhi9' 'xfjfnt' '7jcucc' 'lia9us' 'lhmc0p' 'ghxpx8' '6lcrab'\n",
      " 'clrwli' 'nfp477' 'fns8u8' 'dc9ye1' 'kw1ksh' '6cfgea' 'xht5xh' '82lqqh'] [ 1  5 55  5  3  9  3  8 10  5  5  9  6  3  1  6 13  5  4  6  5  7  5  6\n",
      " 15 12 12 14  6  4  2 22]\n",
      "           id      size                                         image_path  \\\n",
      "index                                                                        \n",
      "2842   uw0k0x    758990  ../raw_data/images/category_0/uw0k0x_758990_1.png   \n",
      "20506  c58ibi  12192768  ../raw_data/images/category_4/c58ibi_12192768_...   \n",
      "29416  g6zfl7  12192768  ../raw_data/images/category_5/g6zfl7_12192768_...   \n",
      "25361  em1fsi  12192768  ../raw_data/images/category_4/em1fsi_12192768_...   \n",
      "9051   csjwb9   6670729  ../raw_data/images/category_1/csjwb9_6670729_1...   \n",
      "...       ...       ...                                                ...   \n",
      "7208   e1bu45   1098828  ../raw_data/images/category_1/e1bu45_1098828_1...   \n",
      "25068  8jsu2u   7803694  ../raw_data/images/category_4/8jsu2u_7803694_1...   \n",
      "8571   hcba7a  12192768  ../raw_data/images/category_1/hcba7a_12192768_...   \n",
      "6331   hdftgu   2073600  ../raw_data/images/category_1/hdftgu_2073600_1...   \n",
      "21243  xc9qm4  12192768  ../raw_data/images/category_4/xc9qm4_12192768_...   \n",
      "\n",
      "      y_cat                                              title  \\\n",
      "index                                                            \n",
      "2842      0                                            Angel❤️   \n",
      "20506     4                                Lamb chop 1 Harry 0   \n",
      "29416     5  Right now we’re in North Central FL, and we ar...   \n",
      "25361     4                         I has supersonic hearing 🐾   \n",
      "9051      1                                 Just keeping watch   \n",
      "...     ...                                                ...   \n",
      "7208      1         Lucy and Lucky having a blast at the park.   \n",
      "25068     4                                          Squad pic   \n",
      "8571      1  Does any one see Greyhound/Lurcher in him? It’...   \n",
      "6331      1  Found barking at strangers protecting his terr...   \n",
      "21243     4              The “Hello Ladies” sleeping position.   \n",
      "\n",
      "                                 Image_url  upvote_ratio  upvotes  upvote_cat  \\\n",
      "index                                                                           \n",
      "2842   https://i.redd.it/dpkytgnec8191.jpg          1.00        1           0   \n",
      "20506  https://i.redd.it/rnkiwmul7i631.jpg          0.99      116           4   \n",
      "29416  https://i.redd.it/qzb2gb0x5ou41.jpg          0.99     1522           5   \n",
      "25361  https://i.redd.it/25k7jp0lan941.jpg          0.97      132           4   \n",
      "9051   https://i.redd.it/yqarowicsfh31.jpg          0.92       10           1   \n",
      "...                                    ...           ...      ...         ...   \n",
      "7208   https://i.redd.it/kt6lorowxr041.jpg          0.90       13           1   \n",
      "25068        https://imgur.com/RtuUvCF.jpg          0.97      100           4   \n",
      "8571   https://i.redd.it/awgbl9bcby551.jpg          0.79        5           1   \n",
      "6331   https://i.redd.it/ldxjrfc79c651.jpg          0.93       12           1   \n",
      "21243  https://i.redd.it/lmoerlopsen91.jpg          0.99      186           4   \n",
      "\n",
      "           year  ...    1    2    3    4    5    6  title_len  \\\n",
      "index            ...                                            \n",
      "2842   1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0          1   \n",
      "20506  0.750000  ...  1.0  0.0  0.0  0.0  0.0  0.0          5   \n",
      "29416  0.833333  ...  0.0  0.0  0.0  1.0  0.0  0.0         55   \n",
      "25361  0.833333  ...  0.0  0.0  1.0  0.0  0.0  0.0          5   \n",
      "9051   0.750000  ...  0.0  0.0  0.0  0.0  0.0  0.0          3   \n",
      "...         ...  ...  ...  ...  ...  ...  ...  ...        ...   \n",
      "7208   0.750000  ...  0.0  0.0  0.0  0.0  0.0  0.0          9   \n",
      "25068  0.666667  ...  0.0  1.0  0.0  0.0  0.0  0.0          2   \n",
      "8571   0.833333  ...  0.0  0.0  0.0  1.0  0.0  0.0         22   \n",
      "6331   0.833333  ...  0.0  0.0  0.0  0.0  0.0  1.0          7   \n",
      "21243  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0          5   \n",
      "\n",
      "                                           preprocessing  \\\n",
      "index                                                      \n",
      "2842                                           [angel❤️]   \n",
      "20506                                [lamb, chop, harry]   \n",
      "29416  [right, north, central, fl, crazy, thunderstor...   \n",
      "25361                              [supersonic, hear, 🐾]   \n",
      "9051                                       [keep, watch]   \n",
      "...                                                  ...   \n",
      "7208                          [lucy, lucky, blast, park]   \n",
      "25068                                       [squad, pic]   \n",
      "8571   [one, see, greyhoundlurcher, take, long, time,...   \n",
      "6331          [find, bark, stranger, protect, territory]   \n",
      "21243                     [hello, lady, sleep, position]   \n",
      "\n",
      "                                               embedding  \\\n",
      "index                                                      \n",
      "2842                                                  []   \n",
      "20506  [[-0.12235965, -0.1281569, -0.3065725, 0.19649...   \n",
      "29416  [[-0.18263498, 1.43723, -0.4116047, -0.6131577...   \n",
      "25361  [[-0.5154052, 0.6335972, 0.011523059, 0.604892...   \n",
      "9051   [[0.37741804, 0.81020796, -0.36291134, -0.4678...   \n",
      "...                                                  ...   \n",
      "7208   [[0.0494106, -0.3409803, -0.36009225, -0.52888...   \n",
      "25068  [[0.07817277, -0.10620472, -0.4206354, 0.22705...   \n",
      "8571   [[0.19927058, 2.0016553, -0.5724674, 0.3240424...   \n",
      "6331   [[-0.46157148, 1.3583232, 0.36058757, 0.801347...   \n",
      "21243  [[-2.0227795, 0.70507616, -0.9979486, -0.18047...   \n",
      "\n",
      "                                                 padding  \n",
      "index                                                     \n",
      "2842   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
      "20506  [[-0.12235965, -0.1281569, -0.3065725, 0.19649...  \n",
      "29416  [[0.21948376, -0.9229505, 0.15131047, -0.02009...  \n",
      "25361  [[-0.5154052, 0.6335972, 0.011523059, 0.604892...  \n",
      "9051   [[0.37741804, 0.81020796, -0.36291134, -0.4678...  \n",
      "...                                                  ...  \n",
      "7208   [[0.0494106, -0.3409803, -0.36009225, -0.52888...  \n",
      "25068  [[0.07817277, -0.10620472, -0.4206354, 0.22705...  \n",
      "8571   [[0.8994805, 1.791743, 0.33340615, -0.28357005...  \n",
      "6331   [[-0.46157148, 1.3583232, 0.36058757, 0.801347...  \n",
      "21243  [[-2.0227795, 0.70507616, -0.9979486, -0.18047...  \n",
      "\n",
      "[30289 rows x 29 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14710/3720163661.py:126: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  X_im_size = dff[\"id\"][idx:end].to_numpy()\n",
      "/tmp/ipykernel_14710/3720163661.py:128: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  X_t_size = dff[\"title_len\"][idx:end].to_numpy()\n",
      "/tmp/ipykernel_14710/3720163661.py:129: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  X_NLP = dff[\"padding\"][idx:end]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for g in GENERATOR:\n",
    "    print(g[0][ \"input_size_im\"], g[0][\"input_size_title\"])\n",
    "    print(g[2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a9318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
