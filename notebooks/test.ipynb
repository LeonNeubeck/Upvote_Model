{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "from PIL import UnidentifiedImageError\n",
    "import requests\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\" # get rid of all tensorflow warnings\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from gensim.models import Word2Vec\n",
    "import datetime as dt\n",
    "import time\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "import pickle\n",
    "import string\n",
    "#### contraction ####\n",
    "### nltk ###\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.utils import pad_sequences\n",
    "#### might be wrong\n",
    "\n",
    "\n",
    "#from models.loader import get_Word2vec\n",
    "\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "vec_size = 40\n",
    "max_length = 10\n",
    "\n",
    "\n",
    "##Davids timestamper\n",
    "\n",
    "\n",
    "def basic(original_df,keep_timestamp=False):\n",
    "    \"\"\"\n",
    "    Transforms 'time_stamp' column from df into individual components 'year',\n",
    "    'month','day','weekday','hour','minute'\n",
    "    \"\"\"\n",
    "    df = original_df.copy()\n",
    "\n",
    "    if 'time_stamp' not in df.columns:\n",
    "        raise ValueError(\"df has no column named 'time_stamp'\")\n",
    "    df['time_stamp'] = pd.to_datetime(df['time_stamp'], unit='s')\n",
    "\n",
    "    df['year'] = df.time_stamp.dt.year\n",
    "    df['month'] = df.time_stamp.dt.month\n",
    "    df['day'] = df.time_stamp.dt.day\n",
    "    df['weekday'] = df.time_stamp.dt.weekday\n",
    "    df['hour'] = df.time_stamp.dt.hour\n",
    "    df['minute'] = df.time_stamp.dt.minute\n",
    "\n",
    "    if keep_timestamp is False:\n",
    "        df = df.drop(columns='time_stamp')\n",
    "    return df\n",
    "\n",
    "def cyclize(original_df):\n",
    "    \"\"\"\n",
    "    Transforms columns named 'month','day','hour','minute' into sin and cos\n",
    "    cyclic values for use with machine learning models\n",
    "    \"\"\"\n",
    "    df = original_df.copy()\n",
    "\n",
    "    need_list = ['month','day','hour','minute']\n",
    "    max_dict = {\n",
    "        'month':12,\n",
    "        'day': 31,\n",
    "        'hour': 23,\n",
    "        'minute': 59\n",
    "    }\n",
    "\n",
    "    for column in need_list:\n",
    "        if column in df.columns:\n",
    "            def sin_trans(number):\n",
    "                return math.sin(number * (2. * math.pi / max_dict[column]))\n",
    "            def cos_trans(number):\n",
    "                return math.cos(number * (2. * math.pi / max_dict[column]))\n",
    "            df['sin_' + column] = df[column].apply(sin_trans)\n",
    "            df['cos_' + column] = df[column].apply(cos_trans)\n",
    "            df = df.drop(columns=column, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode_weekday(original_df, keep_weekday_column=False):\n",
    "    \"\"\"\n",
    "    OneHotEncodes column from df column named 'weekday'\n",
    "    \"\"\"\n",
    "    df = original_df.copy()\n",
    "\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    df_wkdy = pd.DataFrame(enc.fit_transform(df[['weekday']]).toarray())\n",
    "    df = pd.concat([df.reset_index(), df_wkdy], axis=1)\n",
    "    df = df.set_index('index')\n",
    "    if keep_weekday_column==False:\n",
    "        df = df.drop('weekday', axis=1)\n",
    "    return df\n",
    "\n",
    "def transform_timestamp(original_df):\n",
    "    \"\"\"\n",
    "    Takes 'time_stamp' column from df and returns df preprocessed and\n",
    "    ready for machine learning\n",
    "    \"\"\"\n",
    "    df = original_df.copy()\n",
    "    df = basic(df)\n",
    "    df = cyclize(df)\n",
    "    df = encode_weekday(df)\n",
    "    if 'year' in df.columns:\n",
    "        scaler = MinMaxScaler()\n",
    "        df['year'] = scaler.fit_transform(df[['year']].copy())\n",
    "    return df\n",
    "\n",
    "###Binglins NLP\n",
    "\n",
    "\n",
    "def count_len(text):\n",
    "    # add a column to the dataframe, showing the length of each 'title'\n",
    "    text = text.split(' ')\n",
    "    length = len(text)\n",
    "    return length\n",
    "\n",
    "def preprocessing(text, contraction_mapping=CONTRACTION_MAP):\n",
    "\n",
    "    # 1. Expand Contractions\n",
    "    \"\"\"Expand the contractions in English. e.g. I'm ==> I am\"\"\"\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "\n",
    "    # 2. Basic Cleaning\n",
    "    sentence = expanded_text.lower()\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "    ## punctuation dictionary ##\n",
    "    my_punc = string.punctuation\n",
    "    my_punc += '—'\n",
    "    my_punc += '“”’'\n",
    "    ############################\n",
    "    for punctuation in my_punc:\n",
    "        sentence = sentence.replace(punctuation, '')\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    # 2. Remove Stopwords\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    remove_s = \" \".join([word for word in str(sentence).split() if word not in STOPWORDS])\n",
    "\n",
    "    # 3. Word Tokenize\n",
    "    word_tokens = word_tokenize(remove_s)\n",
    "\n",
    "    # 4. Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_n = [lemmatizer.lemmatize(word,pos='n') for word in word_tokens]\n",
    "    lemmatized_v = [lemmatizer.lemmatize(word,pos='v') for word in lemmatized_n]\n",
    "    return lemmatized_v\n",
    "\n",
    "def embedding(text,word2vec):\n",
    "    # 5. Embedding\n",
    "    word2vec,\n",
    "    wv = word2vec.wv\n",
    "    to_array = []\n",
    "    for word in text:\n",
    "        if word in wv.key_to_index:\n",
    "            to_array.append(wv[word])\n",
    "    return np.array(to_array)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#final preprocessor\n",
    "def preprocess(data):\n",
    "    df = data\n",
    "    dataframe=[]\n",
    "\n",
    "    for i in range(6):\n",
    "    # get the path/directory\n",
    "        folder_dir =  f\"images/category_{i}\"\n",
    "        for images in os.listdir(folder_dir):\n",
    "            yeet = []\n",
    "            path = os.path.join(folder_dir, images)\n",
    "            image = Image.open(path)\n",
    "            id_, size, upvotes = images.replace(\".png\", \"\").split(\"_\")\n",
    "            yeet.append(id_)\n",
    "            yeet.append(size)\n",
    "            arr = np.array(image)\n",
    "            try:\n",
    "                A,B,C = arr.shape\n",
    "                if C == 4:\n",
    "                    arr = arr[:,:,:3]\n",
    "                    image = Image.fromarray(arr)\n",
    "                    image.save(path)\n",
    "                yeet.append(path)\n",
    "                yeet.append(i)\n",
    "                dataframe.append(yeet)\n",
    "            except ValueError:\n",
    "                os.remove(path)\n",
    "    data_arrys =pd.DataFrame(dataframe)\n",
    "    data_arrys.rename(columns={0 :'id', 1:\"size\", 2:\"image_path\", 3:\"y_cat\"}, inplace=True)\n",
    "    #merge\n",
    "    df = pd.merge(data_arrys, df)\n",
    "    df = transform_timestamp(df)\n",
    "    ### Add column: length of Title\n",
    "    df['title_len']=df['title'].apply(count_len)\n",
    "    ### Preprocessing ###\n",
    "    df['preprocessing'] = df['title'].apply(lambda sentence: preprocessing(sentence))\n",
    "    ## Embedding ###\n",
    "\n",
    "    word2vec = Word2Vec(sentences=df[\"preprocessing\"], vector_size=vec_size, min_count=10, window=4)####CHANGE DIS\n",
    "    df['embedding'] = df['preprocessing'].apply(lambda x: embedding(x,word2vec))\n",
    "    ### Padding ###\n",
    "\n",
    "    t = pad_sequences(df['embedding'], dtype='float32', padding='post', maxlen=max_length)\n",
    "    tes = []\n",
    "    for i in range(t.shape[0]):\n",
    "        tes.append(t[i])\n",
    "    df['padding'] = tes\n",
    "\n",
    "    X_im = df[\"image_path\"]\n",
    "    df[\"size\"] = df[\"size\"].astype(\"float\")\n",
    "    X_im_size = df[\"size\"]\n",
    "    X_timestep = df[[\"year\", \"sin_month\", \"cos_month\", \"sin_day\", \"cos_day\", \"sin_hour\", \"cos_hour\", \"sin_minute\",\"cos_minute\", 0, 1, 2, 3, 4, 5, 6]].values\n",
    "    X_t_size = df[\"title_len\"]\n",
    "\n",
    "\n",
    "    X_NLP = df[\"padding\"]\n",
    "    X_NLP =[np.expand_dims(x, axis=0) for x in X_NLP]\n",
    "    X_NLP = np.array(X_NLP)\n",
    "    X_NLP = np.concatenate(X_NLP, axis = 0)\n",
    "\n",
    "\n",
    "    df[\"y_cat\"] = df[\"y_cat\"].astype(\"string\")\n",
    "    y = df[\"y_cat\"]\n",
    "    #file_path  = os.path.abspath(os.path.join(os.path.dirname( __file__ ), 'data/processed_df.csv'))\n",
    "    #df.to_csv(file_path)\n",
    "    return { \"input_Im\": X_im, \"input_size_im\": X_im_size, \"input_size_title\": X_t_size,\"input_timestep\":X_timestep,\"input_NLP\": X_NLP}, y, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Multiple Inputs usin https://machinelearningmastery.com/keras-functional-api-deep-learning/\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Masking\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Normalization\n",
    "\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.applications.resnet import ResNet50, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "def initialize_model():\n",
    "    #Image convolution branch\n",
    "    input_Im = Input(shape=(128,128,3), name=\"input_Im\")\n",
    "    conv1 = Conv2D(64, kernel_size=(3, 3),activation='relu')(input_Im)\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
    "    conv2 = Conv2D(32, kernel_size=(3, 3),activation='relu')(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "    conv3 = Conv2D(32, kernel_size=(3, 3),activation='relu')(pool2)\n",
    "    pool3 = MaxPooling2D(pool_size=(2,2))(conv3)\n",
    "    conv2 = Conv2D(16, kernel_size=(3, 3),activation='relu')(pool3)\n",
    "    pool4 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "    flat1 = Flatten()(pool4)\n",
    "\n",
    "    #image_size branch\n",
    "    input_size_im = Input(shape=(1,), name=\"input_size_im\")\n",
    "    hidden1 = Dense(1, activation='relu')(input_size_im)\n",
    "    flat2 = Flatten()(hidden1)\n",
    "\n",
    "\n",
    "\n",
    "    initializer = keras.initializers.VarianceScaling(scale=0.1, mode=\"fan_in\", distribution=\"uniform\")\n",
    "    #NLP branch\n",
    "    input_NLP = Input(shape = (10,40), name=\"input_NLP\")###dont know dim padded and embedded inputs\n",
    "    mask = Masking()(input_NLP)\n",
    "    lstm = LSTM(32, activation = \"tanh\",kernel_initializer=initializer, kernel_regularizer=\"l1\")(mask)\n",
    "    dense1 = Dense(20, activation = \"relu\")(lstm)\n",
    "    flat3 = Flatten()(dense1)\n",
    "\n",
    "    #title_size branch\n",
    "    input_size_title = Input(shape=(1,), name=\"input_size_title\")\n",
    "    layer1 = Dense(1, activation='relu')(input_size_title)\n",
    "    flat4 = Flatten()(layer1)\n",
    "\n",
    "\n",
    "    #normalizer = Normalization()\n",
    "    #normalizer.adapt(\"X_train\")\n",
    "    #davids timestep\n",
    "    input_timestep = Input(shape=(16,),name=\"input_timestep\")#dont know dims\n",
    "    norm = Normalization()(input_timestep)\n",
    "    step1 = Dense(32, activation='relu')(norm)\n",
    "    #drop1 = Dropout(0.3)(step1)\n",
    "    step2 = Dense(16, activation='relu')(step1)\n",
    "    #drop2 = Dropout(0.2)(step2)\n",
    "    step3 = Dense(8, activation='relu')(step2)\n",
    "    #drop3 = Dropout(0.2)(step3)\n",
    "    step4 = Dense(4, activation='relu')(step3)\n",
    "    #drop4 = Dropout(0.2)(step4)\n",
    "    #drop4 = Dropout(0.1)(step5)\n",
    "    flat5 = Flatten()(step4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #concat them\n",
    "    merge = concatenate([flat1, flat2, flat3, flat4, flat5])\n",
    "    final1 = Dense(128, activation='relu')(merge)\n",
    "    final2 = Dense(64, activation='relu')(final1)\n",
    "    final2 = Dense(16, activation='relu')(final1)\n",
    "    output = Dense(6, activation='softmax')(final2)\n",
    "\n",
    "\n",
    "    #final model\n",
    "    model = Model(inputs=[input_Im, input_size_im, input_NLP, input_size_title, input_timestep], outputs=output)\n",
    "\n",
    "    #print(model.summary())\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "def createGenerator(dff, batch_size=BATCH_SIZE):\n",
    "    #dff[\"y_cat\"] = dff[\"y_cat\"].astype(\"string\")\n",
    "    # Shuffles the dataframe, and so the batches as well\n",
    "    dff = dff.sample(frac=1)\n",
    "\n",
    "    # Shuffle=False is EXTREMELY important to keep order of image and coord\n",
    "    flow = datagen.flow_from_dataframe(\n",
    "                                        dataframe=dff,\n",
    "                                        directory=None,\n",
    "                                        x_col=\"image_path\",\n",
    "                                        y_col=\"y_cat\",\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=False,\n",
    "                                        class_mode=\"categorical\",\n",
    "                                        target_size=(128,128),\n",
    "                                        seed=42\n",
    "                                      )\n",
    "    idx = 0\n",
    "    n = len(dff) - batch_size\n",
    "    batch = 0\n",
    "    while True :\n",
    "        # Get next batch of images\n",
    "        X1 = flow.next()\n",
    "        # idx to reach\n",
    "        end = idx + X1[0].shape[0]\n",
    "        # get next batch of lines from df\n",
    "        X_im_size = dff[\"size\"].iloc[idx:end].to_numpy()\n",
    "        X_timestep = dff[[\"year\", \"sin_month\", \"cos_month\", \"sin_day\", \"cos_day\", \"sin_hour\", \"cos_hour\", \"sin_minute\",\"cos_minute\", 0, 1, 2, 3,4, 5, 6]].iloc[idx:end].to_numpy()\n",
    "        X_t_size = dff[\"title_len\"].iloc[idx:end].to_numpy()\n",
    "        X_NLP = dff[\"padding\"].iloc[idx:end]\n",
    "        X_NLP =[np.expand_dims(x, axis=0) for x in X_NLP]\n",
    "        X_NLP = np.array(X_NLP)\n",
    "        X_NLP = np.concatenate(X_NLP, axis = 0)\n",
    "        # Updates the idx for the next batch\n",
    "        idx = end\n",
    "#         print(\"batch nb : \", batch, \",   batch_size : \", X1[0].shape[0])\n",
    "        batch+=1\n",
    "        # Checks if we are at the end of the dataframe\n",
    "        if idx==len(dff):\n",
    "#             print(\"END OF THE DATAFRAME\\n\")\n",
    "            idx = 0\n",
    "        y = X1[1]\n",
    "        X_im = X1[0]\n",
    "        # Yields the image, metadata & target batches\n",
    "        yield { \"input_Im\": X_im, \"input_size_im\": X_im_size, \"input_size_title\": X_t_size,\"input_timestep\":X_timestep,\"input_NLP\": X_NLP},y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def train_model( model_name, new = False, old_model = \"Model_predictor\"):\n",
    "\n",
    "    #file_path  = os.path.abspath(os.path.join(os.path.dirname( __file__ ), ))\n",
    "    df = pd.read_csv('data/balanced_35k.csv', index_col=0)\n",
    "    X_dict, y, df = preprocess(df)\n",
    "    GENERATOR = createGenerator(df)\n",
    "    # model.fit(\n",
    "    # GENERATOR,\n",
    "    # epochs=100,\n",
    "    # batch_size = 32,\n",
    "    # steps_per_epoch=893,\n",
    "    # workers = 1,\n",
    "    # use_multiprocessing=False,\n",
    "\n",
    "    # #validation_data = GENERATOR_train\n",
    "    # )\n",
    "    # loader.save_model(model)\n",
    "    for g in GENERATOR:  \n",
    "        print(g)\n",
    "    pass\n",
    "\n",
    "train_model(\"model_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_stamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_stamp\n",
       "0         112"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>sin_month</th>\n",
       "      <th>cos_month</th>\n",
       "      <th>sin_day</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>sin_hour</th>\n",
       "      <th>cos_hour</th>\n",
       "      <th>sin_minute</th>\n",
       "      <th>cos_minute</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.106293</td>\n",
       "      <td>0.994335</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  sin_month  cos_month   sin_day  cos_day  sin_hour  cos_hour  \\\n",
       "index                                                                      \n",
       "0       0.0        0.5   0.866025  0.201299  0.97953       0.0       1.0   \n",
       "\n",
       "       sin_minute  cos_minute    0  \n",
       "index                               \n",
       "0        0.106293    0.994335  1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('shims': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aeee956cddef7b3b13a13beeafa72459f74aefa818afb83a72cc30b738a1cdcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
